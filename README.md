# Buscador de Subvenciones (Orellana)

<div style="text-align: center;">
  <img src="media/logo.png" alt="alt text" style="width: 250px;" />
</div>

Orellana es una **aplicaci√≥n web** de chat conversacional para buscar informaci√≥n sobre subvenciones (convocatorias, beneficiarios, etc.) en Espa√±a usando servicios de IA y una arquitectura basada en grafos.

## üìã Descripci√≥n

- **Lenguaje**: Python 3.10+
- **Framework Web**: Flask
- **Motor de IA**: Gemini (a trav√©s de `langgraph`) y OpenAI
- **Arquitectura**: Orquestador basado en grafos (LangGraph) con agentes especializados
- **Frontend**: HTML/CSS/JS m√≠nimo, sin frameworks de JS

El objetivo es proporcionar una interfaz de chat donde el usuario haga consultas en lenguaje natural y obtenga respuestas detalladas sobre subvenciones, par√°metros de b√∫squeda, detalles de convocatoria y listados de beneficiarios.

---

## üîß Requisitos

- Python 3.10 o superior
- Acceso a la API de InfoSubvenciones (URL y credenciales en `.env`)
- Claves de API de IA (configuraci√≥n en `.env`)

Instalaci√≥n de dependencias:
```bash
pip install -r requirements.txt
```


## ‚öôÔ∏è Configuraci√≥n

1. Copiar `.env.example` a `.env`:
   ```bash
   cp .env .env.example
   ```
2. Rellenar variables en `.env`:
   ```dotenv
   PORT=5000
   FLASK_DEBUG=True
   INFOSUBVENCIONES_API_URL=https://api.infosubvenciones.gob.es
   INFOSUBVENCIONES_API_KEY=TU_API_KEY
   GEMINI_API_KEY=TU_API_KEY_GEMINI
   ```

---

## üìÇ Estructura de Directorios

```text
buscador_subvenciones_codigo/
‚îú‚îÄ .env
‚îú‚îÄ requirements.txt
‚îú‚îÄ prompts/                   # Plantillas para llamadas a modelos de IA
‚îÇ  ‚îú‚îÄ orchestrator_prompt.txt
‚îÇ  ‚îú‚îÄ extract_params_prompt.txt
‚îÇ  ‚îú‚îÄ extract_years_prompt.txt
‚îÇ  ‚îú‚îÄ convocatoria_extractor_prompt.txt
‚îÇ  ‚îú‚îÄ generate_search_summary_prompt.txt
‚îÇ  ‚îú‚îÄ generate_beneficiaries_summary_prompt.txt
‚îÇ  ‚îú‚îÄ generate_general_response_prompt.txt
‚îÇ  ‚îî‚îÄ generate_detailed_response_prompt.txt
‚îú‚îÄ src/
‚îÇ  ‚îú‚îÄ main.py                # Entrada de la aplicaci√≥n Flask
‚îÇ  ‚îú‚îÄ agents/                # Agentes LLM: extracci√≥n, API, generaci√≥n, errores
‚îÇ  ‚îú‚îÄ services/              # Servicios: llamadas a API, estado de grafo, helpers
‚îÇ  ‚îú‚îÄ graph/                 # Definici√≥n de grafo de flujo de trabajo con LangGraph
‚îÇ  ‚îú‚îÄ templates/
‚îÇ  ‚îÇ   ‚îî‚îÄ index.html         # Plantilla de la interfaz de usuario
‚îÇ  ‚îî‚îÄ static/
‚îÇ      ‚îú‚îÄ css/styles.css     # Estilos b√°sicos
‚îÇ      ‚îî‚îÄ js/main.js         # L√≥gica frontend: env√≠o y renderizado de mensajes
```

---

## üèõÔ∏è Arquitectura & Flujo de Ejecuci√≥n

1. **Usuario** accede a la ruta `/` y carga `index.html`, que inicializa el chat.
2. El cliente JS env√≠a la consulta al endpoint `/api/chat` v√≠a `fetch`.
3. En `main.py`, se crea un objeto `GraphState` con la consulta original y el historial.
4. Se construye el **grafo de flujo** definido en `src/graph/graph.py`:
   - Nodos de **determinaci√≥n de intenci√≥n** y **extracci√≥n de par√°metros** (a√±os, IDs de convocatoria, filtros).
   - Nodos de **llamada a APIs** (`infosubvenciones_service`, `api_caller_agent`).
   - Nodos de **generaci√≥n de respuestas** (res√∫menes de b√∫squeda, respuestas generales, respuestas detalladas, beneficiarios) mediante plantillas de prompts.
   - Nodos de **manejo de errores**.
5. El grafo eval√∫a condiciones en cada arista para decidir la siguiente acci√≥n:
   - `should_extract` ‚Üí extracci√≥n de par√°metros si faltan.
   - `should_call_api` ‚Üí invocar servicio externo.
   - `should_generate_response` ‚Üí formatear la respuesta con IA.
6. Los **Agentes** (`src/agents/*.py`):
   - **ExtractorAgent**: extrae par√°metros via LLM.
   - **ApiCallerAgent**: realiza llamadas HTTP (InfoSubvenciones).
   - **GeneratorAgent**: genera texto de respuesta con LLM.
   - **BeneficiariesAgent**: formatea lista de beneficiarios.
   - **ErrorHandlerAgent**: captura errores y genera mensajes de usuario.
7. El resultado final es **streamed** al cliente (soporte de streaming en `Response(stream_with_context)`), o como texto completo.
8. El frontend renderiza los mensajes en la interfaz de chat.

---

## üõ†Ô∏è Detalle de Componentes

A continuaci√≥n se describen con m√°s detalle los principales ficheros Python del proyecto, su estructura interna, clases, funciones y flujo de datos.

### 1. `src/main.py`
- **Punto de entrada** de la aplicaci√≥n Flask.
- **Importaciones clave**:
  ```python
  from flask import Flask, request, Response, render_template
  from services.langgraph_service import LangGraphService
  from graph.graph import build_conversation_graph
  ```
- **Flask app**:
  ```python
  app = Flask(__name__)
  ```
- **Rutas**:
  - `@app.route('/')`: Renderiza `index.html`.
  - `@app.route('/api/chat', methods=['POST'])`: Recibe JSON `{"message": str}`, carga el historial, construye un grafo v√≠a `build_conversation_graph()`, instancia `LangGraphService` y ejecuta el grafo para generar stream de tokens.
- **Streaming**:
  ```python
  def stream_response(graph_state):
      for chunk in graph_service.execute(graph_state):
          yield chunk
  ```
  Devuelve una respuesta con `Response(stream_with_context(stream_response(state)), mimetype='text/event-stream')`.

---

### 2. Agentes (`src/agents/*.py`)
Cada agente implementa una clase con m√©todo `run(self, state: GraphState) -> GraphState`, recibiendo y devolviendo el estado actualizado.

#### a) `extractor_agent.py`
- **Clase**: `ExtractorAgent`
- **Funci√≥n principal**: extraer par√°metros de la consulta (a√±os, IDs de convocatoria, filtros) usando LLM.
- **M√©todo**: `run(self, state)`:
  1. Prepara prompt basado en plantilla `extract_params_prompt.txt`.
  2. Llama a `llm.generate(prompt)` (puede ser Gemini u OpenAI seg√∫n configuraci√≥n).
  3. Parsea la salida JSON con `json.loads(...)` para actualizar `state.params`.

#### b) `api_caller_agent.py`
- **Clase**: `ApiCallerAgent`
- **Objetivo**: Invocar servicios HTTP y adjuntar resultados en `state.api_response`.
- **M√©todo**: `run(self, state)`:
  1. Lee `state.params` (e.g. `year`, `convocatoria_id`).
  2. Construye URL y cabeceras usando `infosubvenciones_service`.
  3. Ejecuta `requests.get` o `post`, maneja timeouts y errores.
  4. Guarda la respuesta JSON en `state.api_response`.

#### c) `generator_agent.py`
- **Clase**: `GeneratorAgent`
- **Funci√≥n**: Generar la respuesta de usuario en lenguaje natural.
- **M√©todo**: `run(self, state)`:
  1. Selecciona plantilla adecuada (`generate_search_summary_prompt.txt`, `generate_detailed_response_prompt.txt`, etc.) seg√∫n la fase.
  2. Incorpora `state.api_response` y `state.history` en el prompt.
  3. Llama a `llm.stream_generate` para emitir tokens en streaming.

#### d) `beneficiaries_agent.py`
- **Clase**: `BeneficiariesAgent`
- **Prop√≥sito**: Formatear la lista de beneficiarios.
- **M√©todo**: `run(self, state)`:
  1. Recorre `state.api_response["beneficiarios"]`.
  2. Crea strings legibles: "Juan P√©rez (cif: X), 50.000‚Ç¨"
  3. Concatena en un bloque Markdown si se usa streaming.

#### e) `error_handler_agent.py`
- **Clase**: `ErrorHandlerAgent`
- **Objetivo**: Capturar excepciones y producir mensajes claros.
- **M√©todo**: `run(self, state)`:
  1. Detecta `state.error` (excepci√≥n de red, JSON mal formado).
  2. Genera mensaje con plantilla simple: "Lo siento, ha ocurrido un error interno: {detalle}".

---

### 3. Servicios (`src/services/*.py`)
Servicios encapsulan l√≥gica de bajo nivel y helpers.

#### a) `infosubvenciones_service.py`
- **Clase**: `InfoSubvencionesClient`
- **Responsabilidades**:
  - Construir endpoints (e.g. `/convocatorias?year=2023`).
  - A√±adir API key en headers.
  - Funciones p√∫blicas:
    ```python
    def get_convocatorias(self, year: int) -> dict: ...
    def get_convocatoria_details(self, id: str) -> dict: ...
    def get_beneficiarios(self, convocatoria_id: str) -> dict: ...
    ```

#### b) `langgraph_service.py`
- **Clase**: `LangGraphService`
- **M√©todo**: `execute(self, state: GraphState) -> Iterator[str]`.
  1. Instancia el grafo con `state` y nodos.
  2. Itera sobre nodos activos, llamando a `agent.run(state)`.
  3. Sigue aristas seg√∫n condiciones.
  4. Cada vez que un `GeneratorAgent` emita tokens, los yield.

#### c) `graph_state.py`
- **Clase**: `GraphState`
- **Atributos**:
  - `history: List[Dict]` (turnos de chat)
  - `params: Dict[str, Any]` (a√±os, filtros, ids)
  - `api_response: Optional[Dict]`
  - `error: Optional[Exception]`
- **M√©todos auxiliares**:
  - `add_message(role: str, content: str)`
  - `set_error(exc: Exception)`

#### d) `gemini_helpers.py`
- **Funciones**:
  - `configure_llm(api_key: str) -> LLMClient`
  - `stream_llm(prompt: str) -> Iterator[str]`
  - Traducci√≥n de responses chunked de Gemini al formato esperado.

---

### 4. Grafo de Conversaci√≥n (`src/graph/graph.py`)
- **Funci√≥n**: `build_conversation_graph() -> StateGraph`
- **Definici√≥n**:
  1. **Nodos**: Instancias de los Agentes.
  2. **Aristas**: Condiciones lambda sobre `graph_state`; p.ej.:
     ```python
     graph.add_edge(extractor, api_caller, condition=lambda s: not s.params)
     graph.add_edge(api_caller, generator, condition=lambda s: s.api_response)
     ```
  3. **Estado inicial**: `GraphState` con mensaje del usuario.
- **Evaluaci√≥n**: El grafo avanza hasta un nodo `StopNode` tras emitir la respuesta.

---

### 5. Frontend (m√≠nimo JS/Python)
- **`src/templates/index.html`**: Contiene un `<div id="chat">` y un `<form>` para enviar mensajes.
- **`src/static/js/main.js`**:
  1. Captura evento `submit`.
  2. Env√≠a `fetch('/api/chat', { body: JSON.stringify({message}), headers: {'Content-Type':'application/json'} })`.
  3. Lee `response.body` como stream, parsea eventos SSE (EventSource).
  4. Actualiza el DOM con cada fragmento.

---

## üìà Organigrama de la Organizaci√≥n de Agentes

A continuaci√≥n se representa el flujo y las relaciones jer√°rquicas entre los agentes que conforman el orquestador basado en grafos. Usamos una notaci√≥n tipo Mermaid para visualizar los nodos y sus conexiones:

```mermaid
graph TD
    U[Usuario]
    E[ExtractorAgent]
    A[ApiCallerAgent]
    B[BeneficiariesAgent]
    G[GeneratorAgent]
    H[ErrorHandlerAgent]

    U --> E
    E -->|params extra√≠dos| A
    A -->|api_response| G
    G -->|respuesta parcial| B
    B -->|beneficiarios formateados| G
    G --> U

    %% Ruta de errores
    E -->|error detectado| H
    A -->|error HTTP| H
    G -->|error generaci√≥n| H
    H --> U
```

